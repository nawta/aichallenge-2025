from pathlib import Path
import torch
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import hydra
from omegaconf import DictConfig, OmegaConf
from torch.utils.tensorboard import SummaryWriter
from datetime import datetime

from lib.model import TinyLidarNet, TinyLidarNetSmall, TinyLidarNetDeep, TinyLidarNetFusion
from lib.data import MultiSeqConcatDataset
from lib.loss import WeightedSmoothL1Loss



def clean_numerical_tensor(x: torch.Tensor) -> torch.Tensor:
    """NaN, infを安全に除去"""
    if torch.isnan(x).any() or torch.isinf(x).any():
        x = torch.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)
    return x


@hydra.main(config_path="./config", config_name="train", version_base="1.2")
def main(cfg: DictConfig):
    print("------ Configuration ------")
    print(OmegaConf.to_yaml(cfg))
    print("---------------------------")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")

    # Check if using fusion model (requires odometry data)
    use_fusion = cfg.model.name == "TinyLidarNetFusion"
    use_odom = use_fusion or cfg.model.get("use_odom", False)
    
    if use_fusion:
        print("[INFO] Using TinyLidarNetFusion - loading odometry data")

    # Data augmentation settings (defaults: ON)
    augment_mirror = cfg.data.get("augment_mirror", True)
    augment_prob = cfg.data.get("augment_prob", 0.5)
    
    if augment_mirror:
        print(f"[INFO] Mirror augmentation enabled (prob={augment_prob})")
    else:
        print("[INFO] Mirror augmentation disabled")

    # === Dataset ===
    # Training data: apply augmentation based on config
    train_dataset = MultiSeqConcatDataset(
        cfg.data.train_dir, 
        use_odom=use_odom,
        augment_mirror=augment_mirror,
        augment_prob=augment_prob
    )
    # Validation data: no augmentation for fair evaluation
    val_dataset = MultiSeqConcatDataset(
        cfg.data.val_dir, 
        use_odom=use_odom,
        augment_mirror=False,
        augment_prob=0.0
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=cfg.train.batch_size,
        shuffle=True,
        num_workers=cfg.train.num_workers,
        pin_memory=True,
        drop_last=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=cfg.train.batch_size,
        shuffle=False,
        num_workers=cfg.train.num_workers,
        pin_memory=True,
        drop_last=False
    )

    # === Model ===
    if cfg.model.name == "TinyLidarNetSmall":
        model = TinyLidarNetSmall(
            input_dim=cfg.model.input_dim,
            output_dim=cfg.model.output_dim
        ).to(device)
    elif cfg.model.name == "TinyLidarNetDeep":
        model = TinyLidarNetDeep(
            input_dim=cfg.model.input_dim,
            output_dim=cfg.model.output_dim
        ).to(device)
    elif cfg.model.name == "TinyLidarNetFusion":
        state_dim = cfg.model.get("state_dim", 13)
        model = TinyLidarNetFusion(
            input_dim=cfg.model.input_dim,
            state_dim=state_dim,
            output_dim=cfg.model.output_dim
        ).to(device)
    else:
        model = TinyLidarNet(
            input_dim=cfg.model.input_dim,
            output_dim=cfg.model.output_dim
        ).to(device)

    if cfg.train.pretrained_path:
        model.load_state_dict(torch.load(cfg.train.pretrained_path))
        print(f"[INFO] Loaded pretrained model from {cfg.train.pretrained_path}")

    # === Loss & Optimizer ===
    criterion = WeightedSmoothL1Loss(
        steer_weight=cfg.train.loss.steer_weight,
        accel_weight=cfg.train.loss.accel_weight
    )
    optimizer = optim.Adam(model.parameters(), lr=cfg.train.lr)

    # === Logging & Save dirs ===
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = Path(cfg.train.save_dir).expanduser().resolve()
    log_dir = Path(cfg.train.log_dir).expanduser().resolve()
    save_dir.mkdir(parents=True, exist_ok=True)
    log_dir.mkdir(parents=True, exist_ok=True)

    with SummaryWriter(log_dir / timestamp) as writer:
        best_val_loss = float("inf")
        patience_counter = 0
        max_patience = cfg.train.get("early_stop_patience", 10)

        best_path = save_dir / "best_model.pth"
        last_path = save_dir / "last_model.pth"

        # === Training Loop ===
        for epoch in range(cfg.train.epochs):
            model.train()
            train_loss = 0.0

            for batch in tqdm(train_loader, desc=f"[Train] Epoch {epoch+1}/{cfg.train.epochs}"):
                if use_odom:
                    # Fusion model: (scans, odom, targets)
                    scans, odom, targets = batch
                    scans = scans.unsqueeze(1).to(device)
                    odom = odom.to(device)
                    targets = targets.to(device)
                    
                    scans = clean_numerical_tensor(scans)
                    odom = clean_numerical_tensor(odom)
                    targets = clean_numerical_tensor(targets)
                    
                    outputs = model(scans, odom)
                else:
                    # Standard model: (scans, targets)
                    scans, targets = batch
                    scans = scans.unsqueeze(1).to(device)
                    targets = targets.to(device)

                    scans = clean_numerical_tensor(scans)
                    targets = clean_numerical_tensor(targets)

                    outputs = model(scans)
                
                loss = criterion(outputs, targets)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                train_loss += loss.item()

            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = validate(model, val_loader, device, criterion, use_odom=use_odom)

            print(f"Epoch {epoch+1:03d}: Train={avg_train_loss:.4f} | Val={avg_val_loss:.4f}")
            writer.add_scalar("Loss/train", avg_train_loss, epoch + 1)
            writer.add_scalar("Loss/val", avg_val_loss, epoch + 1)

            if avg_val_loss < best_val_loss:
                best_val_loss = avg_val_loss
                torch.save(model.state_dict(), best_path)
                print(f"[SAVE] Best model updated: {best_path} (val_loss={best_val_loss:.4f})")
                patience_counter = 0
            else:
                patience_counter += 1

            torch.save(model.state_dict(), last_path)
            if patience_counter >= max_patience:
                print(f"[EarlyStop] No improvement for {max_patience} epochs.")
                break
    
    print("Training finished.")


def validate(model, loader, device, criterion, use_odom: bool = False):
    model.eval()
    total_loss = 0.0
    with torch.no_grad():
        for batch in tqdm(loader, desc="[Val]", leave=False):
            if use_odom:
                # Fusion model: (scans, odom, targets)
                scans, odom, targets = batch
                scans = scans.unsqueeze(1).to(device)
                odom = odom.to(device)
                targets = targets.to(device)
                
                scans = clean_numerical_tensor(scans)
                odom = clean_numerical_tensor(odom)
                targets = clean_numerical_tensor(targets)
                
                outputs = model(scans, odom)
            else:
                # Standard model: (scans, targets)
                scans, targets = batch
                scans = scans.unsqueeze(1).to(device)
                targets = targets.to(device)
                
                scans = clean_numerical_tensor(scans)
                targets = clean_numerical_tensor(targets)
                
                outputs = model(scans)
            
            loss = criterion(outputs, targets)
            total_loss += loss.item()
    return total_loss / len(loader)


if __name__ == "__main__":
    main()
